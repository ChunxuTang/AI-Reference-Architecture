{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "# from ipywidgets import FloatProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers \n",
    "import torch.optim as optimize\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "#import torchdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c86d5",
   "metadata": {},
   "source": [
    "### Set Up hyperparameters and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb40bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/mnt/alluxio/fuse/yelp-review/yelp_review_sample_large.csv\"\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "num_epochs = 3\n",
    "profiler_enabled = False\n",
    "profiler_log_path = \"../log/nlp-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2230c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99365db4",
   "metadata": {},
   "source": [
    "### Checking device used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2b167",
   "metadata": {},
   "source": [
    "### Build and Preprocess Data Using torchdata.datapipe (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddebf3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torchdata.datapipe to load data, version=0.3.0\n",
    "\n",
    "# datapipe = torchdata.datapipes.iter.FileLister(file_path).filter(\n",
    "#     filter_fn=lambda filename: filename.endswith(\"large.csv\"))\n",
    "\n",
    "# datapipe = torchdata.datapipes.iter.FileOpener(datapipe, mode=\"rt\")\n",
    "# datapipe = datapipe.parse_csv(delimiter = \",\", skip_lines = 1)\n",
    "\n",
    "# _,_,_,score,_,_,_,text,_ = datapipe.unzip(sequence_length=9)\n",
    "# map_score = score.map(classify)\n",
    "\n",
    "# # Lower Reviews for BertTokenizer\n",
    "# def uncase(x):\n",
    "#     return x.lower()\n",
    "\n",
    "# lower_text = text.map(uncase)\n",
    "# clean_datapipe = lower_text.zip(map_score)\n",
    "\n",
    "# Using torchdata.datapipe to load data, version=0.6.0 (tbd)\n",
    "# datapipe = torchdata.datapipes.iter.FileLister(file_path).filter(\n",
    "#     filter_fn=lambda filename: filename.endswith(\".csv\")\n",
    "#     )\n",
    "# datapipe = torchdata.datapipes.iter.FileOpener(datapipe, mode=\"rt\")\n",
    "# datapipe = datapipe.parse_csv(delimiter = \",\", skip_lines = 1)\n",
    "# N_rows = 500\n",
    "\n",
    "# # Drop irrelevant cols\n",
    "# r_datapipe = datapipe.drop([0,1,2,4,5,6,8])\n",
    "\n",
    "# # Classify Stars: 1,2 -> negative; 3,4,5 -> positive\n",
    "# score,text = r_datapipe.unzip(sequence_length=2)\n",
    "# def classify(x):\n",
    "#     if int(x) >2:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "# map_score = score.map(classify)\n",
    "\n",
    "# # Lower Reviews for BertTokenizer\n",
    "# def uncase(x):\n",
    "#     return x.lower()\n",
    "\n",
    "# lower_text = text.map(uncase)\n",
    "# clean_datapipe = lower_text.zip(map_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7915f7",
   "metadata": {},
   "source": [
    "### Build and Preprocess Data Using Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c30434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, max_length):\n",
    "        super(YelpDataset, self).__init__()\n",
    "        \n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.df.iloc[index,7]\n",
    "        text = text.lower()\n",
    "        \n",
    "        label = self.df.iloc[index,3]\n",
    "        label = classify(label)\n",
    "        \n",
    "        \n",
    "        #tokenize,pad and encode reviews\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            max_length=self.max_length,\n",
    "            padding = \"max_length\",\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        encoded_text = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"encoded_text\": torch.tensor(encoded_text, dtype=torch.long),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "# Build Dataset with Bert Tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset= YelpDataset(tokenizer,file_path, max_length=128)\n",
    "\n",
    "# Split Dataset to train, valid and test\n",
    "train_ds, valid_ds,test_ds = random_split(dataset,[8000,1000,1000])\n",
    "\n",
    "# Build DataLoader for train, valid and test Dataset\n",
    "train_dl = DataLoader(dataset = train_ds, num_workers = num_workers, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "valid_dl = DataLoader(dataset = valid_ds, num_workers = num_workers, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246aaba",
   "metadata": {},
   "source": [
    "### Set up model from pretrained BERT for Sequence Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2fe8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        #model_name = \"bert-base-uncased\"\n",
    "        self.encoder = transformers.BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels = 2, \n",
    "            return_dict = True\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids,labels):\n",
    "        \n",
    "        loss, logits = self.encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )[:2]\n",
    "        \n",
    "        return loss,logits\n",
    "\n",
    "# Save model\n",
    "def save_checkpoints(path, model, valid_loss):\n",
    "    \n",
    "    if path == None:\n",
    "        return\n",
    "    state_dict =  {\"model_state_dict\": model.state_dict(),\n",
    "                  \"valid_loss\": valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "    \n",
    "    \n",
    "# Load Model\n",
    "# def load_checkpoints(path, model):\n",
    "#     if path == None:\n",
    "#         return\n",
    "#     state_dict = torch.load(load_path, map_location=device)\n",
    "#     print(f\"Model loaded from {path}\")\n",
    "    \n",
    "#     model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "#     return state_dict[\"valid_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3cdb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT()\n",
    "model = model.to(device)\n",
    "optimizer = optimize.Adam(model.parameters(), lr = 1e-6, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8779169",
   "metadata": {},
   "source": [
    "### Set Up Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b339c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    num_epoch = num_epochs,\n",
    "    train_dl = train_dl,\n",
    "    valid_dl = valid_dl, \n",
    "    model = model, \n",
    "    optimizer = optimizer, \n",
    "    criterion = nn.BCELoss(), \n",
    "    file_path = file_path\n",
    "):\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    print(f\"Started training at the timestamp{start_time}\")\n",
    "          \n",
    "    # Set up metrics\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    total_train_loss = []\n",
    "    total_valid_loss = []\n",
    "    lowest_loss = float(\"Inf\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for  epoch in range(num_epoch):\n",
    "        \n",
    "        for item in tqdm(train_dl, leave = True):\n",
    "            text = item[\"encoded_text\"]\n",
    "            text = text.to(device)\n",
    "            label = item[\"label\"]\n",
    "            label = label.unsqueeze(1)\n",
    "            label = label.to(device)\n",
    "            mask = item[\"mask\"]\n",
    "            mask = mask.to(device)\n",
    "            token_type_ids = item[\"token_type_ids\"]\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            output = model(\n",
    "                text, \n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids, \n",
    "                labels=label\n",
    "            )\n",
    "            \n",
    "            loss = output[0]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update train loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():                    \n",
    "\n",
    "            # validation loop\n",
    "\n",
    "            for item in valid_dl:\n",
    "                text = item[\"encoded_text\"]\n",
    "                text = text.to(device)\n",
    "                label = item[\"label\"]\n",
    "                label = label.unsqueeze(1)\n",
    "                label = label.to(device)\n",
    "                mask = item[\"mask\"]\n",
    "                mask = mask.to(device)\n",
    "                token_type_ids = item[\"token_type_ids\"]\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                output = model(\n",
    "                    text, \n",
    "                    attention_mask=mask,\n",
    "                    token_type_ids=token_type_ids, \n",
    "                    labels=label\n",
    "                )\n",
    "                loss = output[0]\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "        avg_train_loss = train_loss / len(train_dl)\n",
    "        avg_valid_loss = valid_loss / len(valid_dl)\n",
    "        total_train_loss.append(avg_train_loss)\n",
    "        total_valid_loss.append(avg_valid_loss)\n",
    "\n",
    "        # Monitor training progress\n",
    "        print(\"Epoch [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}\"\n",
    "              .format(epoch+1, num_epoch,\n",
    "                      avg_train_loss, avg_valid_loss))\n",
    "\n",
    "       # Save model if valid loss gets lower\n",
    "        if lowest_loss > valid_loss:\n",
    "            lowest_loss = valid_loss\n",
    "            save_checkpoints(file_path + '/' + 'model.pt', model, lowest_loss)\n",
    "\n",
    "        # Reset Metrics\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        if profiler_enabled:\n",
    "            profiler.step()\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "   \n",
    "    print(f\"Started training at the timestamp{end_time}\")\n",
    "    print(f\"Training time in {end_time - start_time:0.4f} seconds\")\n",
    "    \n",
    "    if profiler_enabled:\n",
    "        profiler.stop()\n",
    "        print(\"The profiler is completed. Please open the TensorBoard to browse the metrics.\")\n",
    "    \n",
    "    return total_train_loss, total_valid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b27d34",
   "metadata": {},
   "source": [
    "### Setup Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae58db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = None\n",
    "if profiler_enabled:\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=0, warmup=0, active=1, repeat=1\n",
    "        ),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "            profiler_log_path\n",
    "        ),\n",
    "    )\n",
    "    profiler.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8885f",
   "metadata": {},
   "source": [
    "### Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bee9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    num_epoch = num_epochs, \n",
    "    train_dl = train_dl, \n",
    "    valid_dl = valid_dl, \n",
    "    model = model, \n",
    "    optimizer = optimizer, \n",
    "    criterion = nn.BCELoss(), \n",
    "    file_path = file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e722bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d96103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
