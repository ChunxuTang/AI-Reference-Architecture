{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b604d6ae-0909-402c-81a2-c56627cb22e6",
   "metadata": {},
   "source": [
    "# Model Inference with Alluxio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d24365-13db-4ade-b074-d880428285e8",
   "metadata": {},
   "source": [
    "This notebook shows an example of model inference with Alluxio by classifying some images.\n",
    "\n",
    "Before running this notebook, we need to run the `AI-training-demo.ipynb` file first to train a model and save it to Alluxio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a47e0-7839-4a89-9f14-73880b4bd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c602e25-e624-4925-93c4-e8b31d24fd15",
   "metadata": {},
   "source": [
    "Confirming that the model has been written to the Alluxio FUSE folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52c554-f7e5-4065-a443-5a557c7395c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/alluxio/fuse/models/demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c391f-612d-48eb-b9e8-f7ba70267b82",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Here, we load the Pytorch model from Allxuio.\n",
    "\n",
    "As we use FUSE to mount the Alluxio model into the node, users can conveniently load models from Alluxio as the models are on the local disk.\n",
    "\n",
    "If it outputs \"All keys matched successfully\", it means the model has been loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c5156-9983-4512-9e8b-5216bb0cdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model = models.resnet50(pretrained=False)\n",
    "model_path = \"/mnt/alluxio/fuse/models/demo/ai-demo.pth\"\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275ad1c-515d-48ee-abc4-b5bf91f2f750",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "We prepare some images and classify them via the model trained and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85793a47-e5b4-4e90-b8bb-f4a4f44b9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = ['/mnt/alluxio/fuse/imagenet-mini/val/n01818515/ILSVRC2012_val_00007081.JPEG', \n",
    "               '/mnt/alluxio/fuse/imagenet-mini/val/n02088238/ILSVRC2012_val_00024881.JPEG',\n",
    "               '/mnt/alluxio/fuse/imagenet-mini/val/n02123045/ILSVRC2012_val_00016389.JPEG',\n",
    "               '/mnt/alluxio/fuse/imagenet-mini/val/n01855032/ILSVRC2012_val_00011488.JPEG']\n",
    "\n",
    "images = []\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    images.append(image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = torch.stack(images)\n",
    "    outputs = model(inputs)\n",
    "\n",
    "_, predicted_labels = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b79f6f-bb0e-437f-b5dd-9bf2534e0bb8",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "\n",
    "Here, we load the human-readable lables and plot the sample images with predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c17671-2551-440e-8cd9-8a5e313edef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    class_labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f2fa4-f89f-4722-a089-e4ed12451c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(image_paths), figsize=(12, 4))\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    image = Image.open(image_path)\n",
    "    label = class_labels[predicted_labels[i]]\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_title(label)\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854286d-bae6-4ec4-b926-4d0d54e61d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
